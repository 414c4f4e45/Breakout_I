{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdada48-36c9-4a54-bcc1-92c23fc92425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "\n",
    "class GoogleMapsScraper:\n",
    "    def __init__(self, keyword, lat, lon):\n",
    "        self.keyword = keyword\n",
    "        self.lat = lat\n",
    "        self.lon = lon\n",
    "        self.url = f'https://www.google.com/maps/search/{keyword}/@{lat},{lon}'\n",
    "        self.options = Options()\n",
    "        self.options.add_argument(\"--window-size=1920,1080\")\n",
    "        self.options.add_argument(\"--start-maximized\")\n",
    "        self.options.add_argument(\"--headless\")\n",
    "        self.options.add_argument(\"--disable-gpu\")\n",
    "        self.driver = webdriver.Chrome(options=self.options)\n",
    "        self.driver.maximize_window()\n",
    "        self.res = []\n",
    "\n",
    "    def scroll_page(self):\n",
    "        scrollable_div = self.driver.find_element(By.XPATH, \"//div[@role='feed']\")\n",
    "        scroll_script = \"\"\"\n",
    "        function scrollWithinElement(scrollableDiv) {\n",
    "            return new Promise((resolve, reject) => {\n",
    "                var totalHeight = 0;\n",
    "                var distance = 1000;  // Distance to scroll each time\n",
    "                var scrollDelay = 3000;  // Delay to wait for new content to load\n",
    "                var timer = setInterval(() => {\n",
    "                    var scrollHeightBefore = scrollableDiv.scrollHeight;\n",
    "                    scrollableDiv.scrollBy(0, distance);\n",
    "                    totalHeight += distance;\n",
    "                    \n",
    "                    // If totalHeight is more than the height of the scrollable content\n",
    "                    if (totalHeight >= scrollHeightBefore) {\n",
    "                        totalHeight = 0;\n",
    "                        setTimeout(() => {\n",
    "                            var scrollHeightAfter = scrollableDiv.scrollHeight;\n",
    "                            if (scrollHeightAfter > scrollHeightBefore) {\n",
    "                                // Content loaded, continue scrolling\n",
    "                                scrollHeightBefore = scrollHeightAfter;\n",
    "                            } else {\n",
    "                                // No new content loaded, stop scrolling\n",
    "                                clearInterval(timer);\n",
    "                                resolve();\n",
    "                            }\n",
    "                        }, scrollDelay);\n",
    "                    }\n",
    "                }, 200);\n",
    "            });\n",
    "        }\n",
    "        return scrollWithinElement(arguments[0]);\n",
    "        \"\"\"\n",
    "        self.driver.execute_script(scroll_script, scrollable_div)\n",
    "\n",
    "    def extract_text(self, xpath, element=None):\n",
    "        try:\n",
    "            if element:\n",
    "                return element.find_element(By.XPATH, xpath).text\n",
    "            else:\n",
    "                return self.driver.find_element(By.XPATH, xpath).text\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def extract_attribute(self, xpath, attribute, element=None):\n",
    "        try:\n",
    "            if element:\n",
    "                return element.find_element(By.XPATH, xpath).get_attribute(attribute)\n",
    "            else:\n",
    "                return self.driver.find_element(By.XPATH, xpath).get_attribute(attribute)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def scrape_items(self):\n",
    "        items = self.driver.find_elements(By.XPATH, \"//div[@role='feed']/div/div[@jsaction]\")\n",
    "        for item in items[1:]:\n",
    "            dic = {}\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                self.driver.execute_script(\"arguments[0].scrollIntoView();\", item)\n",
    "                WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable(item))\n",
    "                self.driver.execute_script(\"arguments[0].click();\", item)\n",
    "                time.sleep(2)\n",
    "                try:\n",
    "                    dic['name'] = self.extract_text(\"//div/div/div/div/div/div/div/div/div/div/h1\")\n",
    "                except Exception as e:\n",
    "                    dic['name'] = None\n",
    "                    pass\n",
    "                try:\n",
    "                    dic['rating'] = self.extract_text(\"//div/div/div/div/div/div/div/div/div/div/div/div/div/div[contains(@class,'fontBodyMedium')]/div/span/span\", item)\n",
    "                except Exception:\n",
    "                    dic['rating'] = None\n",
    "                    pass\n",
    "                try:\n",
    "                    dic['address'] = self.extract_text(\"//button[@data-item-id='address']/div/div/div\", item)\n",
    "                except Exception:\n",
    "                    dic['address'] = None\n",
    "                    pass\n",
    "                try:\n",
    "                    dic['phone'] = self.extract_text(\"//button[contains(@data-item-id,'phone')]/div/div/div\", item)\n",
    "                except Exception:\n",
    "                    dic['phone'] = None\n",
    "                    pass\n",
    "                try:\n",
    "                    dic['link'] = self.extract_attribute(\"//div[@role='feed']/div/div[@jsaction]/a\", 'href', item)\n",
    "                except Exception:\n",
    "                    dic['link'] = None\n",
    "                    pass\n",
    "                try:\n",
    "                    dic['website'] = self.extract_text(\"//a[@data-item-id='authority']/div/div/div\", item)\n",
    "                except Exception:\n",
    "                    dic['website'] = None\n",
    "                    pass\n",
    "                try:\n",
    "                    dic['hours'] = self.extract_attribute(\"//div/div/div/div/div/div/div/div/div[contains(@aria-label,'Hide open hours for the week')]\", 'aria-label').replace('\\\\u202f',\" \")\n",
    "                except Exception:\n",
    "                    dic['hours'] = None\n",
    "                    pass\n",
    "                try:\n",
    "                    element_review = item.find_element(By.XPATH,\"//div/button[contains(@aria-label,'Reviews')]\")\n",
    "                    element_review.click()\n",
    "                    time.sleep(2)\n",
    "                    reviews = self.driver.find_elements(By.XPATH,\"//div[@class='MyEned']/span\")\n",
    "                    rws = []\n",
    "                    for index, review in enumerate(reviews[:9]):\n",
    "                        if review.text != 'More':\n",
    "                            rws.append(review.text)\n",
    "                    dic['reviews'] = rws\n",
    "                except Exception as e:\n",
    "                    dic['reviews'] = None\n",
    "                    pass\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            finally:\n",
    "                self.res.append(dic)\n",
    "\n",
    "    def write_files(self, csv_file_name, json_file_name):\n",
    "        csv_headers = list(self.res[0].keys())\n",
    "        with open(csv_file_name, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=csv_headers)\n",
    "            writer.writeheader()\n",
    "            for data in self.res:\n",
    "                data['reviews'] = '; '.join(data['reviews']) if data['reviews'] else ''\n",
    "                writer.writerow(data)\n",
    "        \n",
    "        json_data = {item['name']: item for item in self.res}\n",
    "        \n",
    "        with open(json_file_name, 'w', encoding='utf-8') as jsonfile:\n",
    "            json.dump(json_data, jsonfile, ensure_ascii=False, indent=4)\n",
    "        \n",
    "        print(f\"CSV file '{csv_file_name}' and JSON file '{json_file_name}' have been created.\")\n",
    "\n",
    "    def run(self):\n",
    "        self.driver.get(self.url)\n",
    "        self.scroll_page()\n",
    "        self.scrape_items()\n",
    "        self.driver.quit()\n",
    "\n",
    "\n",
    "scraper = GoogleMapsScraper(keyword='Doctor', lat=22.565290375547733, lon=88.36371897602422)\n",
    "scraper.run()\n",
    "#scraper.write_files('data.csv', 'data.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
